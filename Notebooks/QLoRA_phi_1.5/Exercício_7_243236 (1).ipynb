{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhjrdWXuPMeW"
      },
      "source": [
        "## Module 2 - Fine-tuning Phi-1.5 for sentence classification using QLoRA\n",
        "\n",
        "This notebook presents an example of how to fine-tune Phi-1.5 for sentence classification using QLoRA.\n",
        "\n",
        "QLoRA is a fine-tuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. For more details, please refer to the [QLoRA paper](https://arxiv.org/abs/2106.09647).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgVR534lCXnE"
      },
      "source": [
        "## Aluno: Pedro Rodrigues Corr√™a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dFZ6y7XMXD4"
      },
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install the following libraries:  `transformers`, `datasets`, `torch`, `peft`, `bitsandbytes`, and `trl`.\n",
        "\n",
        "**`transformers`**:\n",
        "\n",
        "Transformers is an open-source library for NLP developed by Hugging Face. It provides state-of-the-art pre-trained models for various NLP tasks, such as text classification, sentiment analysis, question-answering, named entity recognition, etc.\n",
        "\n",
        "**`datasets`**:\n",
        "\n",
        "Datasets is another open-source library developed by Hugging Face that provides a collection of preprocessed datasets for various NLP tasks, such as sentiment analysis, natural language inference, machine translation, and many more.\n",
        "\n",
        "\n",
        "**`torch`**:\n",
        "\n",
        "PyTorch is an open-source machine learning library that provides a wide range of tools and utilities for building and training custom deep learning models. It is already installed in the Colab environment, but we need to install its latest version.\n",
        "\n",
        "**`peft`**:\n",
        "\n",
        "ü§ó PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model‚Äôs parameters. We use PEFT in this example because it supports QLoRA.\n",
        "\n",
        "\n",
        "**`bitsandbytes`**:\n",
        "\n",
        "BitsAndBytes is a library designed to optimize the training of neural networks on modern GPUs. It offers efficient implementations of 8-bit optimizers, which significantly reduce the memory footprint of model parameters and gradients. This reduction in memory usage enables training larger models or using larger batch sizes within the same memory constraints.\n",
        "\n",
        "\n",
        "**`trl`**:\n",
        "\n",
        "ü§ó TRL, or Transfer Learning Library, is a library for training and evaluating transfer learning models. It provides a unified API for training and evaluating various transfer learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrBm7o4LPRb",
        "outputId": "8f22ec12-ba07-43e6-e0a9-d4e5c476b09c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install -q torch\\n!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\\n!pip install datasets\\n!pip install -q peft  # Parameter efficient finetuning - for qLora Finetuning\\n!pip install -q bitsandbytes  # For Model weights quantization\\n!pip install -q trl  # Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''!pip install -q torch\n",
        "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
        "!pip install datasets\n",
        "!pip install -q peft  # Parameter efficient finetuning - for qLora Finetuning\n",
        "!pip install -q bitsandbytes  # For Model weights quantization\n",
        "!pip install -q trl  # Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdm9g9fkjxE"
      },
      "source": [
        "# Setting the device\n",
        "\n",
        "In this example, we will use a GPU to speed up the fine-tuning process. GPUs (Graphics Processing Units) are specialized processors that are optimized for performing large-scale computations in parallel. By using a GPU, we can accelerate the training and inference of a machine learning model, which can significantly reduce the time required to complete these tasks.\n",
        "\n",
        "Before we begin, we need to check whether a GPU is available and select it as the default device for our PyTorch operations. This is because PyTorch can use either a CPU or a GPU to perform computations, and by default, it will use the CPU.\n",
        "\n",
        "For using a GPU in Google Colab:\n",
        "1. Click on the \"Runtime\" menu at the top of the screen.\n",
        "2. From the dropdown menu, click on \"Change runtime type\".\n",
        "3. In the popup window that appears, select \"A100 GPU\" as the hardware accelerator.\n",
        "4. Click on the \"Save\" button.\n",
        "\n",
        "That's it! Now you can use the GPU for faster computations in your notebook.\n",
        "\n",
        "**IMPORTANT**: This example requires a GPU with at least 40GB of memory. If you are using Google Colab, you can select a GPU with 40GB of memory by following the steps above. If you are using a different environment, please make sure that your GPU has at least 40GB of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiV9Y1ZweR2Y",
        "outputId": "357be703-474b-4d1b-e587-13ebcdd384bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Apr 24 20:56:46 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA RTX A6000               Off |   00000000:1E:00.0 Off |                  Off |\n",
            "| 59%   81C    P2            282W /  300W |   14822MiB /  49140MiB |    100%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwYGaELZMwvZ"
      },
      "source": [
        "# Downloading Dataset\n",
        "\n",
        "The SST-2 dataset, or the Stanford Sentiment Treebank, is popular for sentiment analysis tasks in Natural Language Processing (NLP). It consists of movie reviews from the Rotten Tomatoes website that are labeled with either a positive or negative sentiment. The dataset contains 10,662 sentence-level movie reviews, with approximately half of the reviews labeled as positive and the other half labeled as negative. The reviews are also relatively evenly distributed in length, with a median length of 18 tokens.\n",
        "\n",
        "The SST-2 dataset has become a benchmark dataset for sentiment analysis in NLP, and many researchers use it to evaluate the performance of their models. The dataset's popularity is partly due to its high-quality labels and the task's relative simplicity, making it an accessible starting point for researchers and developers new to NLP.\n",
        "\n",
        "In this example, we're using the **`datasets`** library to download and load the training and validation sets of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbX4laO5M6FC",
        "outputId": "7577fd9c-4053-4933-9ed0-bc4e4fb4e7fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the IMDb movie reviews dataset\n",
        "train_dataset = load_dataset('imdb', split='train')\n",
        "test_dataset = load_dataset('imdb', split='test')\n",
        "test_dataset  = test_dataset.shuffle(seed=42).select(range(1000)) # avalia√ß√£o com 1000 amostras aleat√≥rias do conjunto de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OyZ3D_rKEK2"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Now, we will prepare the data for training our model. First, we define a template with the fields `sentence` and `class`. Then, we use the `map` method to apply this template to the dataset. This will create a new dataset with the fields `sentence` and `class` for each example in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBjlqME6VUJn"
      },
      "outputs": [],
      "source": [
        "# Trocar \"sentence\" por \"text\" quando utilizando o IMDb\n",
        "\n",
        "template = \"\"\"Your task is to classify sentences' sentiment as 'positive' or 'negative'. Your answer MUST be ONLY one word, either 'positive' or 'negative'. Sentence:{text}. Answer:{class}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZalJmCxLn9"
      },
      "source": [
        "Before, we need to convert the labels from 0 and 1 to \"negative\" and \"positive\". We can do this by using the `map` method to apply a function to each example in the dataset. The function will take the label as input and return the corresponding string and store in the column `class`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f0d9d6efce654e7ea6bbff6c0f3dfe14",
            "085b9a40fa4e4116b6bc4e78e28f7711",
            "05ad5435945b49f1bf119a11ef42a521",
            "65978617764648069dad78b1b4d0e878",
            "e5cb35dec0fa4b5d9ca0217d5ecc1222",
            "7bc6954fe58d48ceba5feba8c7201f6d",
            "4a9ae76916a94e55baf12ae695b197e9",
            "78b9c727c74049f596116fb091b60b5b",
            "df74223073bc4eb98c4f840779f16c0f",
            "6c8857ea615a482c90fe36436ba79c24",
            "c499cae0308245c1a979547baf9e65ed",
            "ca6bce5f9fb5473dbb22d9f071957d9c",
            "3c44a8ef20a54beb9f8cc5bd475b7f71",
            "da62666e329e40a7835fcfefcca2721d",
            "8b9453f75512474eb453a858662bec1d",
            "72694686f69d43e8bb2563436897b77a",
            "7a3cc4858f3e404ab9432ebb320595bd",
            "038bc33f25684adf8a97cfd3ac0d4d95",
            "a7e6bcc179b648ba85f6a9a0160b4b1a",
            "c5ef3b9707474cfda4e0890a6e9db2d8",
            "c92b126c9dc04403b7fae52fd1984d61",
            "47e497f6d9174a9bb173d03963b3b4fc"
          ]
        },
        "id": "65ew6vQDKEz7",
        "outputId": "c625940b-b262-4bad-81ac-12a2b567d507"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:02<00:00, 11402.47 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "POSITIVE_LABEL = \"positive\"\n",
        "NEGATIVE_LABEL = \"negative\"\n",
        "\n",
        "pattern = re.compile(r'\\s+<.*?>')\n",
        "\n",
        "def preprocess(example):\n",
        "    text  = example[\"text\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    return {\n",
        "        \"text\": text.strip(),\n",
        "        \"class\": POSITIVE_LABEL if label == 1 else NEGATIVE_LABEL\n",
        "    }\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "#train_dataset = train_dataset.map(lambda example: {'class': POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL})\n",
        "train_dataset = train_dataset.map(lambda example: {\"text\": template.format(**example)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE2EQjQ69uLJ"
      },
      "source": [
        "The code below converts the `label` column of the test dataset into a list of strings with `\"positive\"` and `\"negative\"` labels. This is for comparing the model's predictions with the actual labels of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCts-_zF9uLM"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.map(lambda example: {'class': POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3dljkc4QMtR",
        "outputId": "93bf59fe-4159-4bed-fbcb-292211673b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit of rough language & cartoon violence, it's basically okay for older kids (ages 12 & up). As previously stated in the subject line, just sit back,pass the popcorn & just enjoy.\",\n",
              " 'label': 1,\n",
              " 'class': 'positive'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAho1DLxg7z"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-_4YGOYRpmu"
      },
      "source": [
        "## Setting Model Parameters\n",
        "\n",
        "We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H76_erzt1-on"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"phi-1_5-IMDB\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKzTy9MbR2DY"
      },
      "source": [
        "Setting the QLora Parameters\n",
        "\n",
        "1. **lora_r (LoRA attention dimension)**:\n",
        "   - the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
        "\n",
        "2. **lora_alpha (Alpha parameter for LoRA scaling)**:\n",
        "   - This parameter is the LoRA scaling factor applied to the modifications.\n",
        "\n",
        "3. **lora_dropout (Dropout probability for LoRA layers)**:\n",
        "   - This parameter represents the dropout rate applied to the LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXH06fiYybTF"
      },
      "outputs": [],
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64 # @param\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16 # @param\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g69uRG5jzCEZ"
      },
      "source": [
        "Bitsandbytes parameters. These parameters focus on the implementation of 4-bit precision in model loading and computation. Here's an explanation of each:\n",
        "\n",
        "1. **use_4bit (Activate 4-bit precision base model loading)**:\n",
        "   - This parameter, when set to `True`, indicates that the base model (i.e., the pre-trained model or initial model weights) should be loaded using 4-bit precision.\n",
        "2. **bnb_4bit_compute_dtype (Compute dtype for 4-bit base models)**:\n",
        "   - This parameter specifies the data type to be used for computations in the context of 4-bit base models.\n",
        "   - The value `\"float16\"` indicates that computations should be done using 16-bit floating-point numbers.\n",
        "\n",
        "3. **bnb_4bit_quant_type (Quantization type)**:\n",
        "   - This parameter determines the type of quantization to be used for the 4-bit models.\n",
        "   - The options `\"fp4\"` and `\"nf4\"` refer to different quantization schemes.\n",
        "\n",
        "4. **use_nested_quant (Activate nested quantization for 4-bit base models)**:\n",
        "   - When set to `True`, this parameter enables nested quantization for 4-bit base models.\n",
        "   - Nested quantization, often referred to as double quantization, involves applying a second layer of quantization on top of an already quantized model. This can be used for further reducing the model size or for specialized computational optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtraXCCMzJAc"
      },
      "outputs": [],
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True # @param\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\" # @param\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\" # @param [\"nf4\",\"fp4\"]\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTSrnU00F2u"
      },
      "source": [
        "Now, let's define the training arguments.\n",
        "\n",
        "1. **output_dir**:\n",
        "   - Specifies the directory where the model predictions and checkpoints will be stored.\n",
        "\n",
        "2. **num_train_epochs**:\n",
        "   - Sets the number of epochs for training, where one epoch means one pass through the entire training dataset. We set it to `1`\n",
        "\n",
        "3. **fp16, bf16**:\n",
        "   - Enable training with 16-bit floating-point precision (`fp16`) or 16-bit bfloat precision (`bf16`).\n",
        "\n",
        "4. **per_device_train_batch_size**:\n",
        "   - Determines the batch size for training per GPU. This will depend on the GPU used. For an A100, we can use a batch size of 16 examples.\n",
        "\n",
        "5. **per_device_eval_batch_size**:\n",
        "   - Sets the batch size for evaluation per GPU.\n",
        "\n",
        "6. **gradient_accumulation_steps**:\n",
        "   - Indicates the number of update steps over which to accumulate gradients.\n",
        "\n",
        "7. **gradient_checkpointing**:\n",
        "   - When enabled, saves memory by trading compute for memory. Useful for training large models that would otherwise not fit in memory.\n",
        "\n",
        "8. **max_grad_norm (Maximum gradient norm)**:\n",
        "   - Specifies the maximum norm of gradients for gradient clipping, a technique to prevent exploding gradients in deep networks.\n",
        "\n",
        "9. **learning_rate**:\n",
        "   - Sets the initial learning rate for the AdamW optimizer.\n",
        "\n",
        "10. **weight_decay**:\n",
        "    - Specifies the weight decay to apply to all layers except those with bias or LayerNorm weights, as a regularization technique.\n",
        "\n",
        "11. **optim**:\n",
        "    - Defines the optimizer to use, here specified as a variant of AdamW optimized for certain hardware configurations.\n",
        "\n",
        "12. **lr_scheduler_type**:\n",
        "    - Determines the learning rate schedule to use. \"constant\" means the learning rate stays the same throughout training.\n",
        "\n",
        "13. **max_steps**:\n",
        "    - Overrides `num_train_epochs` by setting the number of training steps. If set to a negative value, it's ignored. We set this to `100` to reduce the training time. That means, that our example training does not use the entire traing set.\n",
        "\n",
        "14. **warmup_ratio**:\n",
        "    - Indicates the proportion of total training steps to use for linear warmup of the learning rate.\n",
        "\n",
        "15. **group_by_length**:\n",
        "    - When enabled, sequences are grouped by length into batches. This can save memory and speed up training.\n",
        "\n",
        "16. **save_steps**:\n",
        "    - Determines how often to save a model checkpoint in terms of training steps.\n",
        "\n",
        "17. **logging_steps**:\n",
        "    - Sets the frequency, in terms of training steps, for logging training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayWlSa-s0SJ2"
      },
      "outputs": [],
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\" # @param\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1 # @param\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False # @param\n",
        "bf16 = False # @param\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4 # @param\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4 # @param\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1 # @param\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True # @param\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3 # @param\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 5e-4 # @param\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001 # @param\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\" # @param\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\" # @param\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 1000 # @param\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03 # @param\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True # @param\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 100 # @param\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 100 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypfndp911mbp"
      },
      "source": [
        "Now let's defint the SFTTrainer parameters\n",
        "\n",
        "1. **max_seq_length**:\n",
        "   - This parameter specifies the maximum sequence length to be used.\n",
        "\n",
        "2. **packing**:\n",
        "   - This parameter indicates whether or not to pack multiple short examples into the same input sequence.\n",
        "   - When set to `True`, this technique can be used to increase computational efficiency, particularly in batch processing.\n",
        "\n",
        "3. **device_map**:\n",
        "   - This parameter is a dictionary that maps parts of the model to specific computing devices.\n",
        "   - The entry `{\"\": 0}` specifies that the entire model will be loaded onto GPU 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Yi1p_BR0bA"
      },
      "outputs": [],
      "source": [
        "# Maximum sequence length to use\n",
        "max_seq_length = 512\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269-A6vtSNfg"
      },
      "source": [
        "### Lets Load the base model\n",
        "Let's load the base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg--UCDDZ43f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer # For supervised finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd00Qz6P2LDn"
      },
      "source": [
        "Load the base model with QLoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrBd2aB95aoj"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1G1tGvHS73r",
        "outputId": "09762f05-6c8a-4758-d3a1-62a97dfded7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PhiConfig {\n",
              "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
              "  \"architectures\": [\n",
              "    \"PhiForCausalLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"auto_map\": {\n",
              "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
              "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
              "  },\n",
              "  \"bos_token_id\": null,\n",
              "  \"embd_pdrop\": 0.0,\n",
              "  \"eos_token_id\": null,\n",
              "  \"hidden_act\": \"gelu_new\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 8192,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"model_type\": \"phi\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 24,\n",
              "  \"num_key_value_heads\": 32,\n",
              "  \"partial_rotary_factor\": 0.5,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"qk_layernorm\": false,\n",
              "  \"quantization_config\": {\n",
              "    \"_load_in_4bit\": true,\n",
              "    \"_load_in_8bit\": false,\n",
              "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
              "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "    \"bnb_4bit_quant_type\": \"nf4\",\n",
              "    \"bnb_4bit_use_double_quant\": false,\n",
              "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "    \"llm_int8_has_fp16_weight\": false,\n",
              "    \"llm_int8_skip_modules\": null,\n",
              "    \"llm_int8_threshold\": 6.0,\n",
              "    \"load_in_4bit\": true,\n",
              "    \"load_in_8bit\": false,\n",
              "    \"quant_method\": \"bitsandbytes\"\n",
              "  },\n",
              "  \"resid_pdrop\": 0.0,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"torch_dtype\": \"float16\",\n",
              "  \"transformers_version\": \"4.40.1\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 51200\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zPMcIO2M4S"
      },
      "source": [
        "Load tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5STGmYHw2ORV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "special_tokens_dict = {'additional_special_tokens': ['<s>', '</s>', '[INST]', '[\\INST]']} # Inspirado no Ramon\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQh1-IgyUGvH"
      },
      "source": [
        "# Avalia√ß√£o pr√© Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKJR-CmVwxF"
      },
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "To compute accuracy, we need to define a custom **`string_accuracy`** function since model outputs text rather than numerical values. Therefore, we cannot use the built-in accuracy function directly, which expects numerical values as inputs.\n",
        "\n",
        "The following code defines the **`string_accuracy`** function. It takes two lists of strings as inputs, **`predictions`** and **`references`**. The function computes accuracy by counting the number of predictions that match the corresponding reference and dividing by the total number of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtITggRdVvsD"
      },
      "outputs": [],
      "source": [
        "def string_accuracy(predictions, references):\n",
        "    correct = sum([1 for p, r in zip(predictions, references) if p.lower() == r.lower()])\n",
        "    total = len(predictions)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUYgPcfpV-n6"
      },
      "outputs": [],
      "source": [
        "# Obter a mem√≥ria utilizada da GPU\n",
        "def get_memory():\n",
        "    output = !nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits\n",
        "    return int(output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bff7OaJJUkoy"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "prompt = \"\"\"Your task is to classify sentences' sentiment as 'positive' or 'negative'. Your answer must be ONLY one word, either 'positive' or 'negative', nothing else. Sentence:{text}.Answer:\"\"\"\n",
        "\n",
        "\n",
        "def classify_sentence(model, sentence):\n",
        "  text = prompt.format(text=sentence)\n",
        "  encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(**model_inputs,max_new_tokens=1,bos_token_id=model.config.bos_token_id,\n",
        "                                eos_token_id=model.config.eos_token_id,\n",
        "                                pad_token_id=model.config.eos_token_id\n",
        "                             )\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return tokenizer.decode(outputs[0][len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L6-savpXSNXC",
        "outputId": "3563cfa6-f7ec-44bd-9260-60f747d1836b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' positive'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = 'This is the best movie Ive ever seen.'\n",
        "\n",
        "classify_sentence(base_model, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLAr0dthUOFE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, compute_mem=False, show_most_common_preds=False):\n",
        "    mem_used = 0\n",
        "\n",
        "    predictions = []\n",
        "    predictions_clean = []\n",
        "    references  = dataset[\"class\"]\n",
        "\n",
        "    if compute_mem:\n",
        "        mem_used = get_memory()\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        predicted = classify_sentence(model, item['text'])\n",
        "        predictions.append(predicted)\n",
        "\n",
        "    if compute_mem:\n",
        "        mem_used = abs(get_memory() - mem_used)\n",
        "\n",
        "    for p in predictions:\n",
        "       p = p.strip(' ')\n",
        "       predictions_clean.append(p)\n",
        "\n",
        "    acc = string_accuracy(predictions, references)\n",
        "\n",
        "    if show_most_common_preds:\n",
        "        print(f\"\\nMost common predictions:{Counter(predictions).most_common()}\")\n",
        "\n",
        "    if compute_mem:\n",
        "        return acc, mem_used\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3lmvnj9UP2m",
        "outputId": "5d80d0d7-9bba-44ae-8ada-75a68e49e48f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:58<00:00, 17.14it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Most common predictions:[(' positive', 314), (' negative', 237), (' Negative', 130), (' Positive', 95), ('negative', 82), (' The', 50), (\" '\", 25), (' \"', 16), (' I', 6), ('positive', 4), (' <', 4), (' *', 4), (' **', 3), ('**', 3), (' ***', 2), ('\\n', 2), (' Yes', 2), (' B', 1), (' 13', 1), ('A', 1), (' 1', 1), ('<', 1), (' [', 1), ('_', 1), ('***', 1), (' $', 1), (' This', 1), (' Oz', 1), (' Cool', 1), (' ÔøΩ', 1), (' Anne', 1), (' Plane', 1), (' 10', 1), (' K', 1), (' NO', 1), (' ****', 1), (' a', 1), ('*', 1)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.058, 0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model=base_model, dataset=test_dataset, compute_mem=True, show_most_common_preds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxYdQFqo5aol"
      },
      "source": [
        "## Fine-Tuning with QLoRA and Supervised Fine-Tuning\n",
        "\n",
        "We're ready to fine-tune our model using QLoRA. For this tutorial, we'll use the `SFTTrainer` from the `trl` library.\n",
        "\n",
        "In the context of the code below, `target_modules` refers to specific components or layers of a neural network model that will be modified or adapted using LoRA (Low-Rank Adaptation). LoRA is a technique used to adapt pre-trained models with minimal additional parameters, often used in the context of Transformer models. Here's a breakdown of what each module likely represents:\n",
        "\n",
        "1. **q_proj, k_proj, v_proj, o_proj**:\n",
        "   - These refer to the projections for query (q), key (k), value (v), and output (o) in the attention mechanism of a Transformer model.\n",
        "\n",
        "2. **gate_proj**:\n",
        "   - This refer to a projection layer associated with gating mechanisms in the model, such as those found in Gated Recurrent Units (GRUs) or similar structures.\n",
        "\n",
        "3. **up_proj, down_proj**:\n",
        "   - These refer to projection layers used in upsampling or downsampling within the model.\n",
        "\n",
        "4. **lm_head**:\n",
        "   - This refers to the language model head of a Transformer, which is the final layer that produces the output (like the next word in a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7c8223c4fbdb46788a3fa5c515147b45",
            "2bb0cb14a79e47488c0dd484a9ee3e05",
            "67ad1915273a43dda25d6bf94774fda4",
            "2580e516c96e45e4b06319718646c6a8",
            "dcaa153450a74da79db503849275753b",
            "0895aa1293cd4dfa8fd183f9b9c61982",
            "f224dfa2bef244c499fd63abf44cb77d",
            "4ecd54e8bebc4a7e8cedeb85c4c02773",
            "1187963681ed47ccaf94432703121815",
            "d53aea31e001467a8df29eae3ea85aeb",
            "8b482d5c0b5a4f61bf7535cf78995c85"
          ]
        },
        "id": "nFjA8D0d2oNz",
        "outputId": "f1e5d6c0-2413-45e7-fb68-f6da53635251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:07<00:00, 3559.35 examples/s]\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i187wmtT5aol"
      },
      "source": [
        "## Let's start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "xWxi9tqgEhyZ",
        "outputId": "a89ac8f9-271e-4b8c-fa7b-a217423d7b79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 05:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.964800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.919500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.928900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.898100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.915100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.852200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.861400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.900600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.849400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem√≥ria utilizada durante o treinamento: 12650\n",
            "Tempo de treinamento: 349.346360206604 s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "mem_used = get_memory()\n",
        "start_time = time.time()\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Mem√≥ria utilizada durante o treinamento: {abs(get_memory() - mem_used)}\")\n",
        "print(f\"Tempo de treinamento: {end_time -  start_time} s\")\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2a6U0CuhQLl"
      },
      "source": [
        "# Avalia√ß√£o p√≥s Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyw55yqkhT-V",
        "outputId": "7c5c15f6-91af-4330-f79b-436befcbaf78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:46<00:00, 21.32it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Most common predictions:[('positive', 511), ('negative', 489)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.933, 484)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fine_model = AutoModelForCausalLM.from_pretrained(\n",
        "    new_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "evaluate(model=fine_model, dataset=test_dataset, compute_mem=True, show_most_common_preds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixsfVRz38YA"
      },
      "source": [
        "## Merge the fine-tuned model\n",
        "\n",
        "After fine-tuning, we can merge the fine-tuned model with the base model to get a single model that can be used for inference. This is done by using the PEFT. First, let's clean up the GPU memory by deleting the fine-tuned model. You can also restart the runtime to clear the GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujf17z-7--l5",
        "outputId": "03ca43e0-1cb2-48dd-dd10-4cbf3027d129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20148"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "import gc\n",
        "del base_model\n",
        "del fine_model\n",
        "gc.collect()\n",
        "\n",
        "del trainer\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypNIoSou-_3j"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-cOpLlC_A48",
        "outputId": "aa9485bf-d7b8-4bfe-dc7b-6cdcb6b9aa8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAYoqNyp7m7n"
      },
      "source": [
        "Now, let's load the base model and fine-tuned model and merge them using PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRWhV5WZ3-BO"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "merged_model= PeftModel.from_pretrained(base_model, new_model,)\n",
        "merged_model= merged_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NlvU92-7xnR"
      },
      "source": [
        "Let's save our merged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIYyv_y07zrZ"
      },
      "outputs": [],
      "source": [
        "# Save the merged model\n",
        "merged_model.save_pretrained(\"merged_model_4000\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model_4000\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqJ5l93Yitp1"
      },
      "source": [
        "# Avalia√ß√£o ap√≥s Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx_Ydq81ivwV",
        "outputId": "3ff19c47-6642-48ad-f5dd-a7697e0d9d55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:29<00:00, 33.96it/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Most common predictions:[('positive', 509), ('negative', 491)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.935, 322)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model=merged_model, dataset=test_dataset, compute_mem=True, show_most_common_preds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_hU2be-5aom"
      },
      "source": [
        "## Test the merged model\n",
        "\n",
        "The following code performs the inference stage of the evaluation finetuned Mistral-7B-Instruct model. We define a function called **`classify_sentence`** that is designed to use a pretrained model, likely a variant of a large language model similar to GPT, for sentiment analysis. The description below outlines the steps taken in the function to classify the sentiment of a given sentence as either positive, negative, or possibly neutral. I'll expand on the description by going through the function step-by-step:\n",
        "\n",
        "1. The function accepts a single parameter, `sentence`, which is the text input whose sentiment is to be classified.\n",
        "\n",
        "2. The `sentence` is formatted with the predefined prompt template. This prompt engineering is a common practice when using language models for specific tasks, as it provides context to the model about the task it is supposed to perform.\n",
        "\n",
        "3. The `tokenizer` is applied to the formatted text. Tokenizers convert text into a format that models can understand, which in this case is a series of tokens. The tokenizer is configured to:\n",
        "   - Return tensors compatible with PyTorch (`return_tensors=\"pt\"`).\n",
        "   - Not add special tokens that are usually used to indicate the start and end of a sequence (`add_special_tokens=False`).\n",
        "\n",
        "4. The tokenized input (`encodeds`) is then converted to a PyTorch tensor and moved to the appropriate device (GPU) for inference.\n",
        "\n",
        "5. The inference is performed inside a `torch.no_grad()` context manager, which disables gradient calculations. This is used because we are making predictions, not training the model, and therefore do not need gradients, which would only use extra memory and computational power.\n",
        "\n",
        "6. The `model.generate` function is called to generate a response. This function takes several parameters, such as:\n",
        "   - `**model_inputs`: The tokenized inputs prepared earlier.\n",
        "   - `max_length=8000`: This sets the maximum length of the model's output. The choice of 8000 seems unusually high for sentence classification and might be tailored to specific requirements of the task or the model's capacity.\n",
        "   - `bos_token_id=model.config.bos_token_id`: This specifies the beginning-of-sentence token id, signaling the model where a new sentence starts.\n",
        "   - `eos_token_id=model.config.eos_token_id`: This specifies the end-of-sentence token id, signaling the model where a sentence ends.\n",
        "   - `pad_token_id=model.config.eos_token_id`: This is used for padding shorter sentences to a uniform length. It's unusual to see the end-of-sentence token used for padding, which could be a specific requirement of this model or a mistake.\n",
        "\n",
        "7. After the model generates a response, `torch.cuda.empty_cache()` is called to free up unused memory on the GPU. This is helpful in managing GPU resources, especially when processing multiple requests or dealing with large models.\n",
        "\n",
        "8. Finally, the `tokenizer.decode` function is used to convert the model's output tokens back into human-readable text. The `skip_special_tokens=True` argument removes any special tokens (like padding or end-of-sentence tokens) from the output. The function also skips the input tokens (`outputs[0][len(model_inputs[\"input_ids\"][0]):]`) to only return the newly generated text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28cCRIP4ZsJ"
      },
      "source": [
        "The code below uses the **`classify_sentence`** function to make predictions on the test dataset. We loop through the test dataset and apply the **`classify_sentence`** function to each example. The predictions are stored in a list called **`predictions`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3awVyved9AgW",
        "outputId": "cbafd143-3df3-4551-d1c8-caece506b83e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:28<00:00, 35.28it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "predictions = []\n",
        "references = test_dataset[\"class\"]\n",
        "for item in tqdm(test_dataset):\n",
        "  predicted = classify_sentence(merged_model, item['text'])\n",
        "  predictions.append(predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlWk2IL9L_ay",
        "outputId": "d1e5aa6b-db6f-465c-d714-15685d80a3fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('positive', 509), ('negative', 491)]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(predictions).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXEyUMHwVKJ-"
      },
      "outputs": [],
      "source": [
        "predictions = [\"positive\" if p == \"pos\" else p for p in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SltQDZS9zyt",
        "outputId": "ddc3286e-c963-4354-b4cb-b9af7de1e7d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.935"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = string_accuracy(predictions=predictions, references=references)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "qO0ibw-L-Y8t",
        "outputId": "86ed1bc2-52aa-4acb-8d10-e5559ca8925b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Convert labels to a numerical form\n",
        "labels = {\"positive\": 1, \"negative\": 0}\n",
        "y_true_num = [labels[label] for label in references]\n",
        "y_pred_num = [labels[label] for label in predictions]\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_num, y_pred_num)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "\n",
        "# Labels, title, and ticks\n",
        "ax.set_ylabel('Actual Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoItNVL6CXnS"
      },
      "source": [
        "# Refer√™ncias\n",
        "\n",
        "Ramon Sim√µes Abilio\n",
        "\n",
        "Elton Cardoso do Nascimento"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "038bc33f25684adf8a97cfd3ac0d4d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05ad5435945b49f1bf119a11ef42a521": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b9c727c74049f596116fb091b60b5b",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df74223073bc4eb98c4f840779f16c0f",
            "value": 25000
          }
        },
        "085b9a40fa4e4116b6bc4e78e28f7711": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc6954fe58d48ceba5feba8c7201f6d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4a9ae76916a94e55baf12ae695b197e9",
            "value": "Map:‚Äá100%"
          }
        },
        "0895aa1293cd4dfa8fd183f9b9c61982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1187963681ed47ccaf94432703121815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2580e516c96e45e4b06319718646c6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53aea31e001467a8df29eae3ea85aeb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b482d5c0b5a4f61bf7535cf78995c85",
            "value": "‚Äá25000/25000‚Äá[00:23&lt;00:00,‚Äá1253.03‚Äáexamples/s]"
          }
        },
        "2bb0cb14a79e47488c0dd484a9ee3e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0895aa1293cd4dfa8fd183f9b9c61982",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f224dfa2bef244c499fd63abf44cb77d",
            "value": "Map:‚Äá100%"
          }
        },
        "3c44a8ef20a54beb9f8cc5bd475b7f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a3cc4858f3e404ab9432ebb320595bd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_038bc33f25684adf8a97cfd3ac0d4d95",
            "value": "Map:‚Äá100%"
          }
        },
        "47e497f6d9174a9bb173d03963b3b4fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a9ae76916a94e55baf12ae695b197e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ecd54e8bebc4a7e8cedeb85c4c02773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65978617764648069dad78b1b4d0e878": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8857ea615a482c90fe36436ba79c24",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c499cae0308245c1a979547baf9e65ed",
            "value": "‚Äá25000/25000‚Äá[00:01&lt;00:00,‚Äá18382.51‚Äáexamples/s]"
          }
        },
        "67ad1915273a43dda25d6bf94774fda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ecd54e8bebc4a7e8cedeb85c4c02773",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1187963681ed47ccaf94432703121815",
            "value": 25000
          }
        },
        "6c8857ea615a482c90fe36436ba79c24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72694686f69d43e8bb2563436897b77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b9c727c74049f596116fb091b60b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3cc4858f3e404ab9432ebb320595bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc6954fe58d48ceba5feba8c7201f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8223c4fbdb46788a3fa5c515147b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bb0cb14a79e47488c0dd484a9ee3e05",
              "IPY_MODEL_67ad1915273a43dda25d6bf94774fda4",
              "IPY_MODEL_2580e516c96e45e4b06319718646c6a8"
            ],
            "layout": "IPY_MODEL_dcaa153450a74da79db503849275753b"
          }
        },
        "8b482d5c0b5a4f61bf7535cf78995c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b9453f75512474eb453a858662bec1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92b126c9dc04403b7fae52fd1984d61",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_47e497f6d9174a9bb173d03963b3b4fc",
            "value": "‚Äá25000/25000‚Äá[00:02&lt;00:00,‚Äá10856.23‚Äáexamples/s]"
          }
        },
        "a7e6bcc179b648ba85f6a9a0160b4b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c499cae0308245c1a979547baf9e65ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5ef3b9707474cfda4e0890a6e9db2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c92b126c9dc04403b7fae52fd1984d61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6bce5f9fb5473dbb22d9f071957d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c44a8ef20a54beb9f8cc5bd475b7f71",
              "IPY_MODEL_da62666e329e40a7835fcfefcca2721d",
              "IPY_MODEL_8b9453f75512474eb453a858662bec1d"
            ],
            "layout": "IPY_MODEL_72694686f69d43e8bb2563436897b77a"
          }
        },
        "d53aea31e001467a8df29eae3ea85aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da62666e329e40a7835fcfefcca2721d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7e6bcc179b648ba85f6a9a0160b4b1a",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5ef3b9707474cfda4e0890a6e9db2d8",
            "value": 25000
          }
        },
        "dcaa153450a74da79db503849275753b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df74223073bc4eb98c4f840779f16c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5cb35dec0fa4b5d9ca0217d5ecc1222": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0d9d6efce654e7ea6bbff6c0f3dfe14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_085b9a40fa4e4116b6bc4e78e28f7711",
              "IPY_MODEL_05ad5435945b49f1bf119a11ef42a521",
              "IPY_MODEL_65978617764648069dad78b1b4d0e878"
            ],
            "layout": "IPY_MODEL_e5cb35dec0fa4b5d9ca0217d5ecc1222"
          }
        },
        "f224dfa2bef244c499fd63abf44cb77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}