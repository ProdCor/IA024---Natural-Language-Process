{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência\n",
        "\n",
        "Nome: Pedro Rodrigues Corrêa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "\n",
        "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "\n",
        "Importante:\n",
        "- Deve-se implementar o próprio laço de treinamento.\n",
        "- Implementar o acumulo de gradiente.\n",
        "\n",
        "Dicas:\n",
        "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "\n",
        "- Solução para erro de memória:\n",
        "  - Usar bfloat16 permite quase dobrar o batch size\n",
        "\n",
        "Opcional:\n",
        "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parâmetros"
      ],
      "metadata": {
        "id": "RzG121RHW5VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "lr = 1e-4\n",
        "epochs = 3\n",
        "max_len = 128\n",
        "accumulation_steps = 3"
      ],
      "metadata": {
        "id": "MdQJAIIBW7ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "112e3017-0e32-4d99-8c58-4e154dd9ff06"
      },
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7be8e8d9da90>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510eb8bd-f077-4789-d007-4c463948d1c6"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBM0al6RVXzy",
        "outputId": "25937f99-d3c1-4f9d-e3c8-88757cc4aeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1469a9d8-f71f-4de2-eaa3-075214205347"
      },
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False This film is a perfect example of the recent crop of horror films that simply are not fully realized\n",
            "True This has the funnist jokes out of all the Cheech & Chong flicks. It's the first one I saw with these\n",
            "True what is wrong with you people, if you weren't blown away by the action car sequences and jessica Sim\n",
            "3 últimas amostras treino:\n",
            "False The movie is boring, the characters and scenarios are unrealistic, unbelievable, the action is hilar\n",
            "True This is an above average Jackie Chan flick, due to the fantastic finale and great humor, however oth\n",
            "True A stunning realization occurs when some sort of phenomenon takes place!! Be it, firecrackers going o\n",
            "3 primeiras amostras validação:\n",
            "True Definitely an odd debut for Michael Madsen. Madsen plays Cecil Moe, an alcoholic family man whose li\n",
            "True Probably the worst Dolph film ever. There's nothing you'd want or expect here. Don't waste your time\n",
            "True This bomb is just one 'explosion' after another, with no humor and only absurd situations. Really, p\n",
            "3 últimas amostras validação:\n",
            "False 'The Curse of Frankenstein' sticks faithfully to Mary Shelley's story for one word of the title, whi\n",
            "True \"Addictive\" is an adjective I've heard many times when talking of certain TV shows. Most recently, d\n",
            "False Suppose you've been on a deserted island the last ten years. Haven't heard of Scream and left when H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "d9K9zKV_UF06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = IMDbDataset(x_train, y_train, max_len = max_len)"
      ],
      "metadata": {
        "id": "5PynUpfBeFrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgkRoDkFeR9G",
        "outputId": "c3ed856d-f811-4576-cd82-c82c07585ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  2023,  2143,  2003,  1037,  3819,  2742,  1997,  1996,  3522,\n",
              "         10416,  1997,  5469,  3152,  2008,  3432,  2024,  2025,  3929,  3651,\n",
              "          1012,  2045,  2024,  2048,  5847,  2000,  2202,  1999,  5469,  3152,\n",
              "          1024,  2593,  2017,  2123,  1005,  1056,  2428,  4863,  2054,  1005,\n",
              "          1055,  2183,  2006,  1006,  2030,  2040,  1996,  6359,  2003,  1010,\n",
              "          2066,  1999,  3146,  8859, 10376,  9288,  1007,  2030,  2507,  1996,\n",
              "          3494,  1037,  2843,  1997,  2067,  2466,  1998, 23191,  2061,  2008,\n",
              "          2673,  2003,  4541,  1006, 14414,  2071,  9280,  2022,  2019,  2742,\n",
              "          1997,  2023,  1007,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
              "          1013,  1028,  6854,  1010, 19815, 11896,  1999,  2023,  2181,  1012,\n",
              "          1045,  2156,  7078,  2053,  3114,  2000,  2507,  1037,  2235, 14021,\n",
              "          5596,  1997,  1996,  2067,  2466,  2005,  7010,  2302,  3929, 11847,\n",
              "          1996, 11305,  1997,  2010,  2839,  2030,  2010,   102]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'labels': tensor(0)}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=2).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NK7EH_tX6oB",
        "outputId": "fe7db55a-9938-4442-a74d-9e1c32ac9544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = IMDbDataset(x_train, y_train, max_len=max_len)\n",
        "valid_dataset = IMDbDataset(x_valid, y_valid, max_len=max_len)\n",
        "test_dataset = IMDbDataset(x_test, y_test, max_len=max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "J5B7Ikj_XOeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, eval_dataloader, device = device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct_preds += torch.sum(predicted == labels).item()\n",
        "            total_preds += len(labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(eval_dataloader)\n",
        "    accuracy = correct_preds / total_preds\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "4R1Buy6PXkG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checar se a acurácia antes do treino é de aprox. 50%\n",
        "evaluate(model, valid_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbaftgDaZ-J",
        "outputId": "bbd2639c-48d3-4f6c-c75e-0414bcdb07b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 313/313 [00:50<00:00,  6.24it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7040197356059529, 0.496)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct_preds += torch.sum(predicted == labels).item()\n",
        "            total_preds += len(labels)\n",
        "\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (steps + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        accuracy = correct_preds / total_preds\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "PNynluhAZnu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "train(model, train_dataloader, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrPAV81aaOZQ",
        "outputId": "7c45a362-279e-4df5-e0c3-f461666a968e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training: 100%|██████████| 1250/1250 [05:30<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Training Loss: 0.1244, Training Accuracy: 0.8381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 1250/1250 [05:11<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Training Loss: 0.2312, Training Accuracy: 0.8516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 1250/1250 [05:10<00:00,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - Training Loss: 0.3218, Training Accuracy: 0.8658\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}