{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t-VOYqo25Fd"
      },
      "source": [
        "### Aluno: Pedro Rodrigues Corrêa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ew4bfNG25Fd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6FJBcA025Fe"
      },
      "source": [
        "# Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B6FJ2Kl25Ff"
      },
      "outputs": [],
      "source": [
        "context_size = 9\n",
        "embedding_dim = 128\n",
        "batch_size = 16\n",
        "dropout = 0.1\n",
        "lr = 0.001\n",
        "n_heads = 8\n",
        "n_layer = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4799, 4740)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1 = open(\"pg67724.txt\",\"r\").read()\n",
        "text1_pt = text1.split(\"\\n\\n\")[35:2675]\n",
        "\n",
        "text2 = open(\"pg67725.txt\",\"r\").read()\n",
        "text2_pt = text2.split(\"\\n\\n\")[32:2191]\n",
        "\n",
        "paragraphs = text1_pt + text2_pt\n",
        "\n",
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "len(paragraphs), len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11837"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "\n",
        "UNK = '<UNK>'\n",
        "most_frequent_words = [UNK] + [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 0)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhbhAsZbzJ_J"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to encode a sentence into a list of indices based on a vocabulary\n",
        "def encode_sentence(sentence, vocab):\n",
        "    # Tokenize the sentence into words and punctuation marks\n",
        "    tokens = re.findall(r'\\w+|[.,!?-]', sentence.lower())\n",
        "    # Encode each token using the vocabulary, replacing unknown words with 0\n",
        "    encoded_sentence = [vocab.get(word, 0) for word in tokens]\n",
        "    return encoded_sentence\n",
        "\n",
        "# Function to decode a list of indices into a sentence using a vocabulary\n",
        "def decode_sentence(encoded_sentence, vocab):\n",
        "    words = []\n",
        "    # Iterate through each index in the encoded sentence\n",
        "    for index in encoded_sentence:\n",
        "        # Find the corresponding word in the vocabulary for the index\n",
        "        # If the index is not found in the vocabulary, replace it with \"<UNK>\"\n",
        "        word = next((word for word, code in vocab.items() if code == index), \"<UNK>\")\n",
        "        words.append(word)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGfwvWy525Fh"
      },
      "outputs": [],
      "source": [
        "def gera_input_target(text, context_size):\n",
        "    # Initialize lists to store contexts and targets.\n",
        "    contexts = []\n",
        "    targets = []\n",
        "\n",
        "    for paragraph in text:\n",
        "        text_encoded = encode_sentence(paragraph, vocab)\n",
        "        # Iterate over the text to generate contexts and corresponding targets.\n",
        "        for i in range(len(text_encoded) - context_size):\n",
        "            # Extract the context of size 'context_size' starting from index 'i'.\n",
        "            context = text_encoded[i: i + context_size]\n",
        "            # Retrieve the target element immediately following the context.\n",
        "            target = text_encoded[i + 1: i + context_size +1]\n",
        "            # Append the context and target to their respective lists.\n",
        "            contexts.append(context)\n",
        "            targets.append(target)\n",
        "\n",
        "    # Return the lists of contexts and targets.\n",
        "    return torch.stack((torch.tensor(contexts), torch.tensor(targets)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQb13u0Z25Fh"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, contexts, targets):\n",
        "        # Initialize the dataset with contexts and targets\n",
        "        self.contexts = contexts\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the dataset (number of samples)\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sample at the given index\n",
        "        return self.contexts[idx], self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOsvuMo-25Fh",
        "outputId": "c7686bb8-baf1-427c-ce61-a4dee9e4a439"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pedro\\miniconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "contexts, targets = gera_input_target(cleaned_paragraphs, context_size)\n",
        "X_train, X_test, y_train, y_test = train_test_split(contexts, targets, test_size=0.2, random_state=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB9zAqOE25Fi"
      },
      "outputs": [],
      "source": [
        "# Gera os dataset de treino e validaçãov\n",
        "train_dataset = MyDataset(X_train, y_train)\n",
        "test_dataset = MyDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNFDeJ3z25Fi"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "sample = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItNu1QBb25Fi",
        "outputId": "bf69d9bb-beb5-454f-baff-44900ab37f7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, context_size, embedding_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Initialize positional encoding matrix\n",
        "        self.pe = torch.zeros(context_size, embedding_dim)\n",
        "\n",
        "        # Compute positional encodings\n",
        "        position = torch.arange(0, context_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encodings to input embeddings\n",
        "        if len(x.shape) == 3:\n",
        "            _, seq_len, _ = x.size() # batch size, context size, embedding dim\n",
        "        else:\n",
        "            seq_len, _ = x.size()    # context size, embedding dim\n",
        "\n",
        "        pe = self.pe[:, :seq_len, :]\n",
        "        return x + pe.to(x.device)  # Return input embeddings with positional encodings added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhxJjUxd25Fj"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, context_size, embedding_dim, head_size):\n",
        "        super().__init__()\n",
        "        # Linear transformations for Q, K, V, and output\n",
        "        self.linearWQ = nn.Linear(embedding_dim, head_size, bias = False)\n",
        "        self.linearWK = nn.Linear(embedding_dim, head_size, bias = False)\n",
        "        self.linearWV = nn.Linear(embedding_dim, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape # batch, seq_len, emb_size\n",
        "        k = self.linearWK(x)   # (B,T,C)\n",
        "        q = self.linearWQ(x) # (B,T,C)\n",
        "        v = self.linearWV(x) # (B,T,C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        scores = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        probs = F.softmax(scores, dim=-1) # (B, T, T)\n",
        "\n",
        "        probs  = self.dropout(probs)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        out = probs @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v7nnQaK25Fj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.linearWO = nn.Linear(head_size * num_heads, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.linearWO(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWxiLaDa25Fj"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "            nn.Dropout(dropout)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-3l5W5H25Fj"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, embedding_dim, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = embedding_dim // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(embedding_dim)\n",
        "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HSpQXEC25Fj"
      },
      "outputs": [],
      "source": [
        "class Microtransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_embedding = PositionalEncoding(context_size, embedding_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(embedding_dim, n_head=n_heads) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(embedding_dim) # final layer norm\n",
        "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.embedding(idx) # (B,T,C)\n",
        "        pos_emb = self.positional_embedding(tok_emb) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        #print(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -context_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdTQ5STD25Fj"
      },
      "outputs": [],
      "source": [
        "class Head_ramon(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, context_size, embedding_dim, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,C = x.shape       # (B) batch, (T) context_size, (C) embedding_dim para 1 cabeça e (embedding_dim // n_head) para multiplas cabeças\n",
        "        k     = self.key(x)   # (B,T,C)\n",
        "        q     = self.query(x) # (B,T,C)\n",
        "        v     = self.value(x) # (B,T,C)\n",
        "\n",
        "\n",
        "        # calculando os scores de atenção (\"affinities\")\n",
        "        scores = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        # adicionando máscara causal\n",
        "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        probs  = F.softmax(scores, dim=-1) # (B, T, T)\n",
        "\n",
        "        probs  = self.dropout(probs)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        out = probs @ v       # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jomp1Bx25Fk"
      },
      "outputs": [],
      "source": [
        "class MiniGPT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
        "        super(MiniGPT, self).__init__()\n",
        "\n",
        "        # Embedding layer to convert token indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Positional encoding layer to provide positional information to the model\n",
        "        self.pos_encoding = nn.Embedding(context_size, embedding_dim)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.self_attention_layer = Head(context_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Linear layer for weighted sum after self-attention\n",
        "        self.linearWO = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # First fully connected layer for the feedforward network\n",
        "        self.fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)\n",
        "\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Second fully connected layer for the feedforward network\n",
        "        self.fc2 = nn.Linear(4 * embedding_dim, embedding_dim)\n",
        "\n",
        "        # Layer normalization for the self-attention and feedforward network outputs\n",
        "        self.ln_1 = nn.LayerNorm(embedding_dim)\n",
        "        self.ln_2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        # Layer normalization for the final output\n",
        "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        # Linear layer for outputting logits\n",
        "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout rate should be defined somewhere\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T = input.shape\n",
        "\n",
        "        # Embedding layer\n",
        "        embedding = self.embedding(input)\n",
        "\n",
        "        # Positional encoding\n",
        "        pos_embedding = self.pos_encoding(torch.arange(T, device=device))\n",
        "        x = embedding + pos_embedding\n",
        "\n",
        "        # Self-attention block\n",
        "        x_sa = self.ln_1(x)\n",
        "        x_sa = self.self_attention_layer(x_sa)\n",
        "        x_sa = self.linearWO(x_sa)\n",
        "        x_sa = self.dropout(x_sa)\n",
        "        x = x + x_sa\n",
        "\n",
        "        # Feedforward block\n",
        "        x_ffwd = self.ln_2(x)\n",
        "        x_ffwd = self.fc1(x_ffwd)\n",
        "        x_ffwd = self.relu(x_ffwd)\n",
        "        x_ffwd = self.fc2(x_ffwd)\n",
        "        x_ffwd = self.dropout(x_ffwd)\n",
        "        x = x + x_ffwd\n",
        "\n",
        "        # Final layer normalization\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Output logits\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=10):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # Get the last context_size tokens for the next prediction\n",
        "            idx_cond = idx[:, -context_size:]\n",
        "\n",
        "            # Predict logits for next token\n",
        "            logits = self(idx_cond)   # (B, context_size, vocab_size)\n",
        "\n",
        "            # Focus only on the last time step\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
        "\n",
        "            # Exclude the <unk> token (encoded as 0) by assigning zero probability\n",
        "            probs[:, 0] = 0.0\n",
        "\n",
        "            # Normalize probabilities to ensure sum equals 1\n",
        "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUDH0L-Z25Fk",
        "outputId": "7949d4b2-2876-4ef5-f218-d8c862b7f0d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MiniGPT(\n",
              "  (embedding): Embedding(10001, 128)\n",
              "  (pos_encoding): Embedding(9, 128)\n",
              "  (self_attention_layer): Head(\n",
              "    (linearWQ): Linear(in_features=128, out_features=128, bias=False)\n",
              "    (linearWK): Linear(in_features=128, out_features=128, bias=False)\n",
              "    (linearWV): Linear(in_features=128, out_features=128, bias=False)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (linearWO): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=10001, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MiniGPT(vocab_size, context_size, embedding_dim)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibJeYJxw25Fk",
        "outputId": "04c24200-aea4-427d-b0a9-137010131892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2769553\n"
          ]
        }
      ],
      "source": [
        "def get_num_params(model):\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    return n_params\n",
        "\n",
        "print(get_num_params(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "output = model(input.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6, 6, 7,  ..., 8, 2, 3],\n",
              "        [3, 2, 7,  ..., 8, 2, 5],\n",
              "        [6, 2, 1,  ..., 8, 2, 8],\n",
              "        ...,\n",
              "        [4, 2, 2,  ..., 3, 3, 8],\n",
              "        [2, 2, 2,  ..., 8, 3, 4],\n",
              "        [8, 2, 2,  ..., 3, 2, 5]], device='cuda:0')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   6,   56,  233,   50,    0,    5,    6, 5931,   18],\n",
              "        [6321,    0,    5, 3184,    1,  784,    8,  144,  354],\n",
              "        [ 786,    0,    3,    2, 1070,    4,  102,   10, 1127],\n",
              "        [   2,  776,   32,    3,  562,  629, 1297,    0,    5],\n",
              "        [ 394,  140,  314,    0,    1, 3003,   11,  296,   32],\n",
              "        [ 163,   31,    3,   16,    0, 2380,    4,    0,   89],\n",
              "        [  55,    5,    3,   16,  163,   21,   42,    0,  710],\n",
              "        [   2,   41,    7,   75, 1827,    7,  353,   22,    3],\n",
              "        [3831,    0,   41,    3, 1235,    2,   82,  323,    1],\n",
              "        [   3, 1340,    9, 4174,    4,    7, 2567,   18,  402],\n",
              "        [  24,  393, 3226,  167,  135,    0, 1248,   21,   34],\n",
              "        [ 789,    0,    7,  355, 4277,    4, 1006,    4, 1844],\n",
              "        [   6,   20,   16,  170,    0,    3,  141,  512,    1],\n",
              "        [   6,   22,    3,   69,    6,   27, 3659, 3619,    0],\n",
              "        [   0,    6,  161,   61,  496,  386,    0,    1,   50],\n",
              "        [   9, 4769, 4541,   30,  718,   44,  523,    4,  275]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L",
        "outputId": "b769dad6-df40-4ad0-8b4c-7bdb47e722a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 9.3449, n        Initial Perplexity: 11439.98046875\n",
            "Epoch [1/10],             Loss Treinamento: 4.2310,             PPL Treinamento: 68.7869,             Loss Teste: 3.1590,             PPL Teste: 23.5476\n",
            "Epoch [2/10],             Loss Treinamento: 3.6684,             PPL Treinamento: 39.1897,             Loss Teste: 2.6677,             PPL Teste: 14.4063\n",
            "Epoch [3/10],             Loss Treinamento: 2.6844,             PPL Treinamento: 14.6489,             Loss Teste: 2.4348,             PPL Teste: 11.4133\n",
            "Epoch [4/10],             Loss Treinamento: 3.0766,             PPL Treinamento: 21.6852,             Loss Teste: 2.0025,             PPL Teste: 7.4076\n",
            "Epoch [5/10],             Loss Treinamento: 2.8853,             PPL Treinamento: 17.9089,             Loss Teste: 2.0793,             PPL Teste: 7.9987\n",
            "Epoch [6/10],             Loss Treinamento: 2.3482,             PPL Treinamento: 10.4665,             Loss Teste: 2.0459,             PPL Teste: 7.7364\n",
            "Epoch [7/10],             Loss Treinamento: 2.5213,             PPL Treinamento: 12.4453,             Loss Teste: 2.1142,             PPL Teste: 8.2831\n",
            "Epoch [8/10],             Loss Treinamento: 2.9111,             PPL Treinamento: 18.3766,             Loss Teste: 1.3918,             PPL Teste: 4.0219\n",
            "Epoch [9/10],             Loss Treinamento: 2.0160,             PPL Treinamento: 7.5081,             Loss Teste: 2.1004,             PPL Teste: 8.1695\n",
            "Epoch [10/10],             Loss Treinamento: 1.7124,             PPL Treinamento: 5.5420,             Loss Teste: 1.9151,             PPL Teste: 6.7878\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "model.to(device)\n",
        "\n",
        "# Calculate loss before training\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "initial_loss = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "        logits  = logits.view(B * T, C)\n",
        "        targets = targets.view(B * T)\n",
        "\n",
        "        initial_loss += criterion(logits, targets)\n",
        "\n",
        "    avg_loss = initial_loss / len(train_loader)\n",
        "\n",
        "initial_PPL = torch.exp(avg_loss)\n",
        "print(f'Initial Loss: {avg_loss:.4f}, n\\\n",
        "        Initial Perplexity: {initial_PPL}')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "        logits  = logits.view(B * T, C)\n",
        "        targets = targets.view(B * T)\n",
        "\n",
        "        loss_train = criterion(logits, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_train.item()\n",
        "\n",
        "        ppl_train = torch.exp(loss_train)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(inputs)\n",
        "\n",
        "            B, T, C = logits.shape\n",
        "            logits  = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "    loss_test = criterion(logits, targets)\n",
        "    ppl_test = torch.exp(loss_test)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "            Loss Treinamento: {loss_train.item():.4f}, \\\n",
        "            PPL Treinamento: {ppl_train.item():.4f}, \\\n",
        "            Loss Teste: {loss_test.item():.4f}, \\\n",
        "            PPL Teste: {ppl_test.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut3i2MMG25Fl",
        "outputId": "c4c14d19-2522-488a-d98b-611676bf3a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "não\n",
            "truão\n",
            "da\n",
            "vossa\n",
            "laia\n",
            "és\n",
            "pery\n",
            "do\n",
            "vencedor\n",
            "da\n",
            "guerra\n",
            "não\n",
            "contra\n",
            "essa\n",
            "esperança\n",
            "que\n",
            "poder\n",
            "repellia\n",
            "a\n",
            "embriaguez\n",
            "do\n",
            "prazer\n",
            "como\n",
            "a\n",
            "espada\n",
            "conteve\n"
          ]
        }
      ],
      "source": [
        "input = \"não\"\n",
        "idx = torch.tensor(encode_sentence(input, vocab), dtype=torch.long).unsqueeze(0)\n",
        "idx = idx.to(device)\n",
        "idx = model.generate(idx, 25)\n",
        "output = decode_sentence(idx[0].tolist(), vocab)\n",
        "\n",
        "for palavra in output:\n",
        "    print(palavra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOCGCzV25Fl"
      },
      "source": [
        "# Referências\n",
        "\n",
        "Ramon Simões Abilio"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}